{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验六：决策树分类器\n",
    "- 姓名：冯思程\n",
    "- 学号：2112213\n",
    "- 专业：计算机科学与技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验要求\n",
    "### 截止日期：12月15日\n",
    "以学号+姓名(6)的命名形式打包实验代码+实验报告，发送到邮箱18329300691@163.com\n",
    "\n",
    "### 数据集\n",
    "\n",
    "这里使用的是给定的数据集。\n",
    "\n",
    "\n",
    "### 基本要求\n",
    "- \t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "- \t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "### 中级要求\n",
    "-   对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "-   对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "### 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================\n",
    "\n",
    "# 开始\n",
    "\n",
    "**环境**：python 3.10.9+vscode 1.82.2+一些必备的第三方库，例如pandas等。\n",
    "\n",
    "<span style=\"color:red\">**注意**</span>：我在后文的代码都补充了适当的注释并在代码前进行了适当注释和分析，感谢学长学姐的批阅！辛苦！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础要求部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    编号  色泽  根蒂  敲声  纹理 好瓜\n",
      "0    1  青绿  蜷缩  浊响  清晰  是\n",
      "1    2  乌黑  蜷缩  沉闷  清晰  是\n",
      "2    3  乌黑  蜷缩  浊响  清晰  是\n",
      "3    4  青绿  蜷缩  沉闷  清晰  是\n",
      "4    5  浅白  蜷缩  浊响  清晰  是\n",
      "5    6  青绿  稍蜷  浊响  清晰  是\n",
      "6    7  乌黑  稍蜷  浊响  稍糊  是\n",
      "7    8  乌黑  稍蜷  浊响  清晰  是\n",
      "8    9  乌黑  稍蜷  沉闷  稍糊  否\n",
      "9   10  青绿  硬挺  清脆  清晰  否\n",
      "10  11  浅白  硬挺  清脆  模糊  否\n",
      "11  12  浅白  蜷缩  浊响  模糊  否\n",
      "12  13  青绿  稍蜷  浊响  稍糊  否\n",
      "13  14  浅白  稍蜷  沉闷  稍糊  否\n",
      "14  15  浅白  蜷缩  浊响  模糊  否\n",
      "15  16  青绿  蜷缩  沉闷  稍糊  否\n"
     ]
    }
   ],
   "source": [
    "train1 = pd.read_csv(\"Watermelon-train1.csv\", encoding='gbk')\n",
    "test1 = pd.read_csv(\"Watermelon-test1.csv\", encoding='gbk')\n",
    "train2 = pd.read_csv(\"Watermelon-train2.csv\", encoding='gbk')\n",
    "test2 = pd.read_csv(\"Watermelon-test2.csv\", encoding='gbk')\n",
    "print(train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    色泽  根蒂  敲声  纹理  好瓜\n",
      "0    2   0   0   2   1\n",
      "1    1   0   1   2   1\n",
      "2    1   0   0   2   1\n",
      "3    2   0   1   2   1\n",
      "4    0   0   0   2   1\n",
      "5    2   1   0   2   1\n",
      "6    1   1   0   1   1\n",
      "7    1   1   0   2   1\n",
      "8    1   1   1   1   0\n",
      "9    2   2   2   2   0\n",
      "10   0   2   2   0   0\n",
      "11   0   0   0   0   0\n",
      "12   2   1   0   1   0\n",
      "13   0   1   1   1   0\n",
      "14   0   0   0   0   0\n",
      "15   2   0   1   1   0\n",
      "    色泽  根蒂  敲声  纹理     密度  好瓜\n",
      "0    2   0   0   2  0.697   1\n",
      "1    1   0   1   2  0.774   1\n",
      "2    1   0   0   2  0.634   1\n",
      "3    2   0   1   2  0.608   1\n",
      "4    0   0   0   2  0.556   1\n",
      "5    2   1   0   2  0.403   1\n",
      "6    1   1   0   1  0.481   1\n",
      "7    1   1   0   2  0.437   1\n",
      "8    1   1   1   1  0.666   0\n",
      "9    2   2   2   2  0.243   0\n",
      "10   0   2   2   0  0.245   0\n",
      "11   0   0   0   0  0.343   0\n",
      "12   2   1   0   1  0.639   0\n",
      "13   0   1   1   1  0.657   0\n",
      "14   1   1   0   2  0.360   0\n",
      "15   0   0   0   0  0.593   0\n",
      "16   2   0   1   1  0.719   0\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(data):\n",
    "    # 删除第一列-标号列\n",
    "    data = data.drop(columns=['编号'])\n",
    "    # 定义映射关系\n",
    "    feature_mappings = {\n",
    "        '色泽': {'浅白': 0, '乌黑': 1, '青绿': 2},\n",
    "        '根蒂': {'蜷缩': 0, '稍蜷': 1, '硬挺': 2},\n",
    "        '敲声': {'浊响': 0, '沉闷': 1, '清脆': 2},\n",
    "        '纹理': {'模糊': 0, '稍糊': 1, '清晰': 2},\n",
    "        '好瓜': {'是': 1, '否': 0}\n",
    "    }\n",
    "\n",
    "    # 将特征和目标变量进行转换\n",
    "    for column, mapping in feature_mappings.items():\n",
    "        if column in data.columns:\n",
    "            data[column] = data[column].map(mapping)\n",
    "    \n",
    "    return data\n",
    "\n",
    "train1 = preprocessing(train1)\n",
    "test1 = preprocessing(test1)\n",
    "train2 = preprocessing(train2)\n",
    "test2 = preprocessing(test2)\n",
    "print(train1)\n",
    "print(train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现ID3决策树：\n",
    "\n",
    "在信息论与概率统计中，熵是表示随机变量不确定性的量。X是⼀个取值为有限个的离散随机变量，熵的公式如下：\n",
    "$$ H(X)=-\\sum_{i=1}^{n} p\\left(x_{i}\\right) \\log p\\left(x_{i}\\right)$$ \n",
    "$𝐻(𝑋)$就被称作随机变量𝑋的熵，它表示随机变量不确定的度量。熵取值越大，随机变量不确定性越大。当随机变量为均匀分布时，熵最大。当某一状态概率取值为1时，熵的值为零。\n",
    "\n",
    "条件熵表示在已知随机变量𝑋的条件下随机变量𝑌的不确定性，定义为给定𝑋条件下𝑌的条件概率分布的熵对𝑋的数学期望:\n",
    "$$H(Y \\mid X)=\\sum_{x} p(x) H(Y \\mid X=x) =-\\sum_{x} p(x) \\sum_{y} p(y \\mid x) \\log p(y \\mid x)$$\n",
    "\n",
    "特征𝐴对数据集𝐷的**信息增益**就是熵$𝐻(𝐷)$与条件熵$𝐻(𝐷|𝐴)$之差:\n",
    "$$𝐻(𝐷)−𝐻(𝐷∣𝐴)$$\n",
    "\n",
    "表示已知特征𝐴的信息而使得数据集𝐷的信息不确定减少的程度。信息增益越大的特征代表其具有更强的分类能力，所以我们就要**选择能够使数据的不确定程度减少最多的特征**，也就是信息增益最大的特征。\n",
    "\n",
    "#### 决策树的生成\n",
    "\n",
    "从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "\n",
    "#### 决策树的停止条件：\n",
    "\n",
    "  1. 当前结点所有样本都属于同⼀类别；\n",
    "  2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "  3. 达到最大树深；\n",
    "  4. 达到叶子结点的最小样本数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息熵\n",
    "def calculate_entropy(dataframe, target_column='好瓜'):\n",
    "    \n",
    "    num_entries = len(dataframe)\n",
    "    label_counts = dataframe[target_column].value_counts()\n",
    "\n",
    "    entropy = 0.0\n",
    "    for count in label_counts:\n",
    "        probability = count / num_entries\n",
    "        entropy -= probability * log(probability, 2)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# 从一个数据集中划分出一个子数据集，这个子数据集只包含在指定特征上具有特定值的数据行，并且在返回的数据点中不包括这个特定特征轴的值。\n",
    "def split_dataframe(dataframe, feature, value):\n",
    "\n",
    "    # 筛选出特征列符合指定值的行\n",
    "    filtered_df = dataframe[dataframe[feature] == value]\n",
    "\n",
    "    # 删除该特征列\n",
    "    subdataframe = filtered_df.drop(columns=[feature])\n",
    "\n",
    "    return subdataframe\n",
    "\n",
    "\n",
    "# 将遍历每个特征列，计算每个特征的信息增益，最后返回具有最大信息增益的特征列的名称。\n",
    "def id3_choose_best_feature_to_split(dataframe, target_column='好瓜'):\n",
    "    \n",
    "    base_entropy = calculate_entropy(dataframe, target_column)\n",
    "    best_info_gain = 0.0\n",
    "    best_feature = ''\n",
    "\n",
    "    for feature in dataframe.columns:\n",
    "        if feature == target_column:\n",
    "            continue  # Skip the target column\n",
    "        unique_vals = dataframe[feature].unique()\n",
    "        new_entropy = 0.0\n",
    "        \n",
    "        for value in unique_vals:\n",
    "            sub_df = split_dataframe(dataframe, feature, value)\n",
    "            probability = len(sub_df) / float(len(dataframe))\n",
    "            new_entropy += probability * calculate_entropy(sub_df, target_column)\n",
    "        \n",
    "        info_gain = base_entropy - new_entropy\n",
    "        print(f\"ID3: Information Gain for feature '{feature}': {info_gain:.3f}\")\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature\n",
    "\n",
    "\n",
    "# 当数据集的所有特征都已处理，但类别标签仍不唯一时，使用多数表决的方法决定叶子节点的分类。\n",
    "def majority_vote(labels):\n",
    "    label_count = {}\n",
    "    for label in labels:\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "\n",
    "    sorted_label_count = sorted(label_count.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_label_count[0][0]\n",
    "\n",
    "\n",
    "# 实现基于ID3决策树的构建\n",
    "def ID3_createTree(dataset, labels):\n",
    "    classList = dataset.iloc[:, -1].tolist()  # 假设最后一列为类别列\n",
    "\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "\n",
    "    if len(dataset.columns) == 1:\n",
    "        return majority_vote(classList)\n",
    "\n",
    "    bestFeatLabel = id3_choose_best_feature_to_split(dataset, dataset.columns[-1])\n",
    "    ID3Tree = {bestFeatLabel: {}}\n",
    "\n",
    "    # 删除已使用的特征\n",
    "    remaining_features = [lbl for lbl in dataset.columns if lbl != bestFeatLabel and lbl != dataset.columns[-1]]\n",
    "\n",
    "    uniqueVals = dataset[bestFeatLabel].unique()\n",
    "    for value in uniqueVals:\n",
    "        reducedDataset = split_dataframe(dataset, bestFeatLabel, value)\n",
    "        ID3Tree[bestFeatLabel][value] = ID3_createTree(reducedDataset, remaining_features)\n",
    "\n",
    "    return ID3Tree\n",
    "\n",
    "\n",
    "# 单个数据实例（DataFrame 的一行）进行分类\n",
    "def classify(decision_tree, feature_labels, test_vector):\n",
    "    if not isinstance(decision_tree, dict):\n",
    "        # 如果decision_tree不是字典，那么它是一个叶节点的值\n",
    "        return decision_tree\n",
    "\n",
    "    root_feature = list(decision_tree.keys())[0]\n",
    "    sub_tree = decision_tree[root_feature]\n",
    "\n",
    "    if root_feature not in feature_labels:\n",
    "        return None  # 如果特征标签不在feature_labels中，返回None\n",
    "\n",
    "    feature_index = feature_labels.index(root_feature)\n",
    "    \n",
    "    if feature_index >= len(test_vector):\n",
    "        return None  # 如果特征索引超出范围，返回None\n",
    "\n",
    "    feature_value = test_vector[feature_index]\n",
    "\n",
    "    if isinstance(sub_tree, dict) and feature_value in sub_tree:\n",
    "        # 只有当sub_tree是字典时才执行比较\n",
    "        if isinstance(sub_tree[feature_value], dict):\n",
    "            return classify(sub_tree[feature_value], feature_labels, test_vector)\n",
    "        else:\n",
    "            return sub_tree[feature_value]\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 对一个DataFrame的每行进行分类。\n",
    "def classifytest(decision_tree, feature_labels, test_data):\n",
    "    classification_results = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        classification_results.append(classify(decision_tree, feature_labels, row))\n",
    "    return classification_results\n",
    "\n",
    "\n",
    "# 计算准确率\n",
    "def calculate_accuracy(predicted_labels, true_labels):\n",
    "    if len(true_labels) == 0:\n",
    "        return 0  # 如果没有真实标签，返回0作为准确率\n",
    "    correct = sum(p == t for p, t in zip(predicted_labels, true_labels))\n",
    "    accuracy = correct / len(true_labels)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3: Information Gain for feature '色泽': 0.174\n",
      "ID3: Information Gain for feature '根蒂': 0.148\n",
      "ID3: Information Gain for feature '敲声': 0.180\n",
      "ID3: Information Gain for feature '纹理': 0.503\n",
      "ID3: Information Gain for feature '色泽': 0.138\n",
      "ID3: Information Gain for feature '根蒂': 0.544\n",
      "ID3: Information Gain for feature '敲声': 0.544\n",
      "ID3: Information Gain for feature '色泽': 0.322\n",
      "ID3: Information Gain for feature '根蒂': 0.073\n",
      "ID3: Information Gain for feature '敲声': 0.322\n",
      "ID3: Information Gain for feature '根蒂': 0.000\n",
      "ID3: Information Gain for feature '敲声': 1.000\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "# 构建决策树\n",
    "feature_labels = list(train1.columns)  # 所有特征的列名\n",
    "decision_tree1 = ID3_createTree(train1, feature_labels)\n",
    "\n",
    "# 对测试数据进行分类\n",
    "predicted_labels1 = classifytest(decision_tree1, feature_labels, test1)\n",
    "\n",
    "# 计算准确率\n",
    "true_labels = test1.iloc[:, -1].tolist()  # 真实标签\n",
    "accuracy = calculate_accuracy(predicted_labels1, true_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中级要求部分\n",
    "\n",
    "这里我选择用到的是C4.5算法，下面进行一下简单的介绍：\n",
    "\n",
    "- C4.5算法与ID3算法相似，其对ID3算法进行了改进。\n",
    "- 信息增益作为划分准则存在的问题：\n",
    "\n",
    "     信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用**信息增益比**来选择特征，可以校正这个问题。\n",
    "     \n",
    "- 特点\n",
    "  - 能够完成对连续属性的离散化处理\n",
    "  - 能够对不完整数据进行处理\n",
    "  - 需要对数据集进行多次的顺序扫描和排序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现C4.5算法的最佳特征列选取\n",
    "def C45_chooseBestFeatureToSplit(dataframe):\n",
    "    \n",
    "    base_entropy = calculate_entropy(dataframe)\n",
    "    best_info_gain_ratio = 0.0\n",
    "    best_feature = ''\n",
    "\n",
    "    for feature in dataframe.columns[:-1]:  # 遍历所有特征，排除最后的类别列\n",
    "        feat_list = dataframe[feature]\n",
    "        unique_vals = set(feat_list)  # 创建唯一的分类标签列表\n",
    "        new_entropy = 0.0\n",
    "        IV = 0.0\n",
    "\n",
    "        for value in unique_vals:\n",
    "            sub_dataframe = split_dataframe(dataframe, feature, value)\n",
    "            p = len(sub_dataframe) / float(len(dataframe))\n",
    "            new_entropy += p * calculate_entropy(sub_dataframe)\n",
    "            IV -= p * log(p, 2) if p > 0 else 0  # 防止p为0导致的log(0)错误\n",
    "\n",
    "        info_gain = base_entropy - new_entropy\n",
    "        info_gain_ratio = info_gain / IV if IV != 0 else 0\n",
    "\n",
    "        print(f\"C4.5: Information Gain Ratio for feature '{feature}': {info_gain_ratio:.3f}\")\n",
    "        if info_gain_ratio > best_info_gain_ratio:\n",
    "            best_info_gain_ratio = info_gain_ratio\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature\n",
    "\n",
    "# 实现基于C4.5决策树的构建\n",
    "def C45_createTree(dataset, labels):\n",
    "    classList = dataset.iloc[:, -1].tolist()  # 假设最后一列为类别列\n",
    "\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "\n",
    "    # 如果没有更多特征可以用于进一步划分\n",
    "    if len(dataset.columns) == 1:\n",
    "        return majority_vote(classList)\n",
    "\n",
    "    bestFeatLabel = C45_chooseBestFeatureToSplit(dataset)\n",
    "    C45Tree = {bestFeatLabel: {}}\n",
    "\n",
    "    # 删除已使用的特征\n",
    "    remaining_features = [lbl for lbl in labels if lbl != bestFeatLabel]\n",
    "\n",
    "    uniqueVals = dataset[bestFeatLabel].unique()\n",
    "    for value in uniqueVals:\n",
    "        reducedDataset = split_dataframe(dataset, bestFeatLabel, value)\n",
    "        C45Tree[bestFeatLabel][value] = C45_createTree(reducedDataset, remaining_features)\n",
    "\n",
    "    return C45Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5: Information Gain Ratio for feature '色泽': 0.068\n",
      "C4.5: Information Gain Ratio for feature '根蒂': 0.102\n",
      "C4.5: Information Gain Ratio for feature '敲声': 0.106\n",
      "C4.5: Information Gain Ratio for feature '纹理': 0.263\n",
      "C4.5: Information Gain Ratio for feature '密度': 0.244\n",
      "C4.5: Information Gain Ratio for feature '色泽': 0.031\n",
      "C4.5: Information Gain Ratio for feature '根蒂': 0.339\n",
      "C4.5: Information Gain Ratio for feature '敲声': 0.270\n",
      "C4.5: Information Gain Ratio for feature '密度': 0.241\n",
      "C4.5: Information Gain Ratio for feature '色泽': 0.274\n",
      "C4.5: Information Gain Ratio for feature '敲声': 0.000\n",
      "C4.5: Information Gain Ratio for feature '密度': 0.579\n",
      "C4.5: Information Gain Ratio for feature '色泽': 0.212\n",
      "C4.5: Information Gain Ratio for feature '根蒂': 0.101\n",
      "C4.5: Information Gain Ratio for feature '敲声': 0.332\n",
      "C4.5: Information Gain Ratio for feature '密度': 0.311\n",
      "C4.5: Information Gain Ratio for feature '色泽': 1.000\n",
      "C4.5: Information Gain Ratio for feature '根蒂': 0.000\n",
      "C4.5: Information Gain Ratio for feature '密度': 1.000\n",
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# 构建决策树\n",
    "feature_labels = list(train2.columns)  # 所有特征的列名\n",
    "decision_tree2 = C45_createTree(train2, feature_labels)\n",
    "\n",
    "# 对测试数据进行分类\n",
    "predicted_labels2 = classifytest(decision_tree2, feature_labels, test2)\n",
    "\n",
    "# 计算准确率\n",
    "true_labels2 = test2.iloc[:, -1].tolist()  # 真实标签\n",
    "accuracy2 = calculate_accuracy(predicted_labels2, true_labels2)\n",
    "\n",
    "print(\"Accuracy:\", accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级要求部分\n",
    "\n",
    "- 决策树很容易出现**过拟合现象**。原因在于学习时完全考虑的是如何提⾼对训练数据的正确分类从⽽构建出过于复杂的决策树。\n",
    "- 解决这个问题的方法称为**剪枝**，即对已生成的树进行简化。具体地，就是从已生成的树上裁剪掉⼀些子树或叶节点，并将其根节点或父节点作为新的叶节点。 \n",
    "- 决策树的剪枝基本策略有**预剪枝 (Pre-Pruning)** 和 **后剪枝 (Post-Pruning)**\n",
    "   - **预剪枝**：是根据⼀些原则**极早的停止树增长**，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。 \n",
    "   - **后剪枝**：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后**自底向上**的对树中所有的非叶结点进⾏逐一考察 。\n",
    "\n",
    "这里我对在基础要求中实现的ID3决策树和在中级要求中实现的C4.5决策树来进行预剪枝和后剪枝策略的添加，并进行分析。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3_createTree2(dataset, labels, test_dataset=None, pre_pruning=True, post_pruning=True):\n",
    "    classList = dataset.iloc[:, -1].tolist()  # 假设最后一列为类别列\n",
    "\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "\n",
    "    if len(dataset.columns) == 1:\n",
    "        return majority_vote(classList)\n",
    "\n",
    "    bestFeatLabel = id3_choose_best_feature_to_split(dataset)\n",
    "    ID3Tree = {bestFeatLabel: {}}\n",
    "    \n",
    "    uniqueVals = dataset[bestFeatLabel].unique()\n",
    "\n",
    "    # 预剪枝逻辑\n",
    "    if pre_pruning and test_dataset is not None:\n",
    "        leaf = majority_vote(classList)\n",
    "        accuracy_without_split = calculate_accuracy(\n",
    "            classifytest({bestFeatLabel: leaf}, labels, test_dataset),\n",
    "            test_dataset.iloc[:, -1]\n",
    "        )\n",
    "        accuracy_with_split = 0\n",
    "        for value in uniqueVals:\n",
    "            subDataset = split_dataframe(dataset, bestFeatLabel, value)\n",
    "            subTestset = split_dataframe(test_dataset, bestFeatLabel, value)\n",
    "            if not subTestset.empty:  # 确保子测试集不为空\n",
    "                subTree = ID3_createTree2(subDataset, labels[:], subTestset)\n",
    "                accuracy_with_split += calculate_accuracy(\n",
    "                    classifytest({bestFeatLabel: {value: subTree}}, labels, subTestset),\n",
    "                    subTestset.iloc[:, -1]\n",
    "                )\n",
    "        if accuracy_without_split >= accuracy_with_split / len(uniqueVals):\n",
    "            print(\"发生预剪枝处理\")\n",
    "            return leaf\n",
    "\n",
    "    # 构建子树\n",
    "    for value in uniqueVals:\n",
    "        subDataset = split_dataframe(dataset, bestFeatLabel, value)\n",
    "        if test_dataset is not None:  # 检查test_dataset是否为None\n",
    "            subTestset = split_dataframe(test_dataset, bestFeatLabel, value)\n",
    "            if not subTestset.empty:  # 确保子测试集不为空\n",
    "                subTree = ID3_createTree2(subDataset, labels[:], subTestset, pre_pruning, post_pruning)\n",
    "                accuracy_with_split += calculate_accuracy(\n",
    "                    classifytest({bestFeatLabel: {value: subTree}}, labels, subTestset),\n",
    "                    subTestset.iloc[:, -1]\n",
    "                )\n",
    "        else:\n",
    "            subTestset = None\n",
    "        ID3Tree[bestFeatLabel][value] = ID3_createTree2(subDataset, labels[:], subTestset, pre_pruning, post_pruning)\n",
    "    # 后剪枝逻辑\n",
    "    if post_pruning and test_dataset is not None:\n",
    "        leaf = majority_vote(classList)\n",
    "        accuracy_without_split = calculate_accuracy(\n",
    "            classifytest({bestFeatLabel: leaf}, labels, test_dataset),\n",
    "            test_dataset.iloc[:, -1]\n",
    "        )\n",
    "        accuracy_with_split = calculate_accuracy(\n",
    "            classifytest(ID3Tree, labels, test_dataset),\n",
    "            test_dataset.iloc[:, -1]\n",
    "        )\n",
    "        if accuracy_without_split >= accuracy_with_split:\n",
    "            print(\"发生后剪枝处理\")\n",
    "            return leaf\n",
    "\n",
    "    return ID3Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3: Information Gain for feature '色泽': 0.254\n",
      "ID3: Information Gain for feature '根蒂': 0.150\n",
      "ID3: Information Gain for feature '敲声': 0.227\n",
      "ID3: Information Gain for feature '纹理': 0.319\n",
      "ID3: Information Gain for feature '色泽': 0.128\n",
      "ID3: Information Gain for feature '根蒂': 0.592\n",
      "ID3: Information Gain for feature '敲声': 0.592\n",
      "ID3: Information Gain for feature '色泽': 0.918\n",
      "ID3: Information Gain for feature '根蒂': 0.252\n",
      "ID3: Information Gain for feature '敲声': 0.918\n",
      "发生预剪枝处理\n",
      "ID3: Information Gain for feature '色泽': 0.128\n",
      "ID3: Information Gain for feature '根蒂': 0.592\n",
      "ID3: Information Gain for feature '敲声': 0.592\n",
      "ID3: Information Gain for feature '色泽': 0.128\n",
      "ID3: Information Gain for feature '根蒂': 0.592\n",
      "ID3: Information Gain for feature '敲声': 0.592\n",
      "ID3: Information Gain for feature '色泽': 0.918\n",
      "ID3: Information Gain for feature '根蒂': 0.252\n",
      "ID3: Information Gain for feature '敲声': 0.918\n",
      "发生预剪枝处理\n",
      "ID3: Information Gain for feature '色泽': 0.918\n",
      "ID3: Information Gain for feature '根蒂': 0.252\n",
      "ID3: Information Gain for feature '敲声': 0.918\n",
      "发生预剪枝处理\n",
      "Accuracy 2.0: 0.8\n"
     ]
    }
   ],
   "source": [
    "# 构建决策树\n",
    "feature_labels = list(train1.columns)  # 所有特征的列名\n",
    "\n",
    "# 分割数据集，比例为 70% 训练，30% 测试\n",
    "train, test = train_test_split(train1, test_size=0.3, random_state=2023)\n",
    "decision_tree3 = ID3_createTree2(train, feature_labels, test)\n",
    "\n",
    "# 对测试数据进行分类\n",
    "predicted_labels3 = classifytest(decision_tree3, feature_labels, test1)\n",
    "\n",
    "# 计算准确率\n",
    "true_labels = test1.iloc[:, -1].tolist()  # 真实标签\n",
    "accuracy = calculate_accuracy(predicted_labels3, true_labels)\n",
    "\n",
    "print(\"Accuracy 2.0:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C45_createTree2(dataframe, feature_labels, test_dataframe=None, pre_pruning=True, post_pruning=True):\n",
    "    class_list = dataframe.iloc[:, -1].tolist()\n",
    "\n",
    "    if len(set(class_list)) == 1:\n",
    "        return class_list[0]\n",
    "\n",
    "    if len(dataframe.columns) == 1:\n",
    "        return majority_vote(class_list)\n",
    "\n",
    "    best_feature = C45_chooseBestFeatureToSplit(dataframe)\n",
    "    C45_tree = {best_feature: {}}\n",
    "    unique_values = dataframe[best_feature].unique()\n",
    "\n",
    "    # 预剪枝逻辑\n",
    "    if pre_pruning and test_dataframe is not None:\n",
    "        leaf = majority_vote(class_list)\n",
    "        leaf_predictions = [leaf] * len(test_dataframe)\n",
    "        root_accuracy = calculate_accuracy(leaf_predictions, test_dataframe.iloc[:, -1].tolist())\n",
    "        split_accuracies = []\n",
    "\n",
    "        for value in unique_values:\n",
    "            sub_dataframe = split_dataframe(dataframe, best_feature, value)\n",
    "            sub_test_dataframe = split_dataframe(test_dataframe, best_feature, value)\n",
    "            if not sub_test_dataframe.empty:\n",
    "                sub_tree = C45_createTree2(sub_dataframe, feature_labels, sub_test_dataframe, pre_pruning, post_pruning)\n",
    "                sub_predictions = classifytest({best_feature: sub_tree}, feature_labels, sub_test_dataframe)\n",
    "                split_accuracies.append(calculate_accuracy(sub_predictions, sub_test_dataframe.iloc[:, -1].tolist()))\n",
    "\n",
    "        if split_accuracies and all(accuracy < root_accuracy for accuracy in split_accuracies):\n",
    "            print(\"发生预剪枝处理\")\n",
    "            return leaf\n",
    "\n",
    "    # 构建子树\n",
    "    for value in unique_values:\n",
    "        sub_dataframe = split_dataframe(dataframe, best_feature, value)\n",
    "        if not sub_dataframe.empty:\n",
    "            sub_test_dataframe = split_dataframe(test_dataframe, best_feature, value) if test_dataframe is not None else None\n",
    "            C45_tree[best_feature][value] = C45_createTree2(sub_dataframe, feature_labels, sub_test_dataframe, pre_pruning, post_pruning)\n",
    "\n",
    "    # 后剪枝逻辑\n",
    "    if post_pruning and test_dataframe is not None:\n",
    "        leaf = majority_vote(class_list)\n",
    "        tree_predictions = classifytest(C45_tree, feature_labels, test_dataframe)\n",
    "        tree_accuracy = calculate_accuracy(tree_predictions, test_dataframe.iloc[:, -1].tolist())\n",
    "        leaf_accuracy = calculate_accuracy([leaf] * len(test_dataframe), test_dataframe.iloc[:, -1].tolist())\n",
    "\n",
    "        if leaf_accuracy > tree_accuracy:\n",
    "            print(\"发生后剪枝处理\")\n",
    "            return leaf\n",
    "\n",
    "    return C45_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5: Information Gain Ratio for feature '色泽': 0.152\n",
      "C4.5: Information Gain Ratio for feature '根蒂': 0.113\n",
      "C4.5: Information Gain Ratio for feature '敲声': 0.118\n",
      "C4.5: Information Gain Ratio for feature '纹理': 0.257\n",
      "C4.5: Information Gain Ratio for feature '密度': 0.273\n",
      "发生后剪枝处理\n",
      "Accuracy 2.0: 0.6\n"
     ]
    }
   ],
   "source": [
    "# 构建决策树\n",
    "feature_labels = list(train2.columns)  # 所有特征的列名\n",
    "\n",
    "# 分割数据集，比例为 70% 训练，30% 测试\n",
    "trainc, testc = train_test_split(train2, test_size=0.3, random_state=2023)\n",
    "decision_tree4 = C45_createTree2(trainc, feature_labels, testc)\n",
    "\n",
    "# 对测试数据进行分类\n",
    "predicted_labels4 = classifytest(decision_tree4, feature_labels, test2)\n",
    "\n",
    "# 计算准确率\n",
    "true_labels = test2.iloc[:, -1].tolist()  # 真实标签\n",
    "accuracy = calculate_accuracy(predicted_labels4, true_labels)\n",
    "\n",
    "print(\"Accuracy 2.0:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "\n",
    "可以看到我在加入预剪枝和后剪枝策略后ID3决策树和C45决策树的分类精度变化如下：\n",
    "- ID3决策树从0.7提升到0.8\n",
    "- C45决策树从0.6到0.6，没有变化\n",
    "\n",
    "对于ID3决策树，我通过我加入的标记发现了经过了三次预剪枝的处理，精度从0.7提升到0.8，查看test1的数量可以发现是多了一个分类正确的数据行，说明预剪枝确实提升了决策树的分类效果，有效地避免掉了过拟合的发生。\n",
    "\n",
    "对于C45决策树，我通过我加入的标记发现了经过了一次后剪枝的处理，但是精度并没有发生变化，这里我仔细探究认为是与数据量有关，后剪枝处理是整个决策树构建后进行遍历来进行剪枝的，所以理论上是可以剪掉可能过拟合的子树，但是这里精度并没有发生变化，我觉得是由于test2数据量太小，test2只有5个数据行，偶然性太大，如果数据量足够，决策效果会有明显的提升。\n",
    "\n",
    "额外分析说明：其实在实现加入预剪枝和后剪枝算法后的ID3和C45决策树的训练集组成已经发生了改变，原来是完全的一个train全作为训练集进行决策树的搭建，现在是将train划分成了训练和测试进行决策树构建，所以其实这已经不是一个十分完美的控制变量法的对比了。这里我还考虑了如果我保持train不变，而是从test中取一部分过来进行预剪枝和后剪枝的测试，但是后来我否定了这种思路，因为这会在测试集上造成更大的偏差，我保证了在测试的时候都用了统一的测试集。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
